# https://github.com/sonatype/nxrm3-ha-repository/tree/main/nxrm-ha
# https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-deployment-options/option-2---on-premises-high-availability-deployment-using-kubernetes#Option2OnPremisesHighAvailabilityDeploymentUsingKubernetes-helmlic

# LIMITATION:
#   Can't (or need to be careful) use special characters such as double-quotes and dollar mark.

#$ export HELM_NAME="sonatype-helm" NAME_SPACE="nexusrepo" RELEASE_NAME="nxrm3ha" ADMIN_PWD="admin123" LICENSE_B64="$(base64 -i $HOME/share/sonatype/sonatype-license.lic)" DB_SERVER="192.168.4.31";export NFS_SERVER="192.168.4.31" SHARE_DIR="/var/tmp/share/sonatype/${RELEASE_NAME}-nfs"
#$ export TAG="3.72.0"
# NOTE: to decode: kubectl get secrets nexus-repo-license.lic -n nexusrepo -o jsonpath="{.data.nexus-repo-license\.lic}" | base64 -d >/tmp/my-test2.lic

## Most likely only once
#$ #kubectl create namespace ${NAME_SPACE}
### https://github.com/sonatype/nxrm3-ha-repository/blob/main/nxrm-ha/README.md#configuration-for-dynamic-persistent-volume-provisioning-2
#$ helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner --set nfs.server=${NFS_SERVER} --set nfs.path=${SHARE_DIR} --set storageClass.create=true
### Setting up NFS PV and PVC (used in `additionalVolumes`)
#$ eval "echo \"$(cat ./k8s-nfs-pv-pvc-tmpl.yaml | grep -v '^\s*#')\"" > ./${RELEASE_NAME}-nfs-pv-pvc.yaml
#$ kubectl apply -f ./${RELEASE_NAME}-nfs-pv-pvc.yaml -n ${NAME_SPACE}
### Adding sonatype helm repo (and search for versions to confirm)
#$ helm repo add ${HELM_NAME} https://sonatype.github.io/helm3-charts/
#$ helm search repo ${HELM_NAME}/nxrm-ha --versions | head

# Install / reinstall / upgrade:
#$ helm repo update ${HELM_NAME}
#$ #helm uninstall ${RELEASE_NAME} -n ${NAME_SPACE}
#$ eval "echo \"$(cat ./helm-nxrm3ha-values.yml | grep -v '^\s*#')\"" > ./${RELEASE_NAME}_values.yaml
#$ helm upgrade -i ${RELEASE_NAME} ${HELM_NAME}/nxrm-ha -f ./${RELEASE_NAME}_values.yaml -n ${NAME_SPACE} #--version 72.0.0  --debug --dry-run


# For troubleshooting:
#$ git -C ~/IdeaProjects/nxrm3-ha-repository pull && git -C ~/IdeaProjects/nxrm3-ha-repository fetch
#$ helm upgrade ${RELEASE_NAME} ~/IdeaProjects/nxrm3-ha-repository/nxrm-ha -f ./${RELEASE_NAME}_values.yaml --dry-run > ${RELEASE_NAME}_dry-run.yaml
#$ helm template nxrm3ha sonatype-helm/nxrm-ha -f ./helm-nxrm3ha-values.yml -n sonatype-helm >./helm-nxrm3ha-deploying.yaml
#$ kubectl -n sonatype-helm apply -f ./helm-nxrm3ha-deploying.yaml


# Seems namespaces broken?
namespaces:
  nexusNs:
    enabled: false
    name: ${NAME_SPACE}

statefulset:
  name: nxrm-statefulset
  serviceName: nxrm-statefulset-service
  replicaCount: 1
  hostAliases:
   - ip: "192.168.4.31"
     hostnames:
     - "nxiqha-k8s.standalone.localdomain"
     - "nxiqha-admin-k8s.standalone.localdomain"
  clustered: true
  additionalVolumeMounts:
    - name: nexus-deploy-volume
      mountPath: /opt/sonatype/nexus/deploy
    - name: nexus-blobs-volume
      mountPath: /nexus-data/blobs
  additionalVolumes:
    - name: nexus-deploy-volume
      persistentVolumeClaim:
        claimName: ${RELEASE_NAME}-deploy-nfs-pv-claim
    - name: nexus-blobs-volume
      persistentVolumeClaim:
        claimName: ${RELEASE_NAME}-blobs-nfs-pv-claim
  initContainer:
    args:
      - -c
      - >-
        mkdir -p /nexus-data/etc/logback &&
        mkdir -p /nexus-data/log/tasks &&
        mkdir -p /nexus-data/log/audit &&
        touch -a /nexus-data/log/tasks/allTasks.log &&
        touch -a /nexus-data/log/audit/audit.log &&
        touch -a /nexus-data/log/request.log &&
        find /nexus-data -mindepth 0 -maxdepth 3 -type d -not -path "*/blobs/*" ! -user 200 | xargs -P3 -I{} -t chown 200:200 {}
  #     Not expecting "blobs" directory under this location, so maxdepth 3 would be OK
  container:
    image:
      repository: sonatype/nexus3
      #nexusTag: ${TAG:-latest}  # From 3.66.0, this is no longer required
    resources:
      # See help documentation, these are minimum system requirements
      requests:
        cpu: 2
        memory: '4Gi'
      limits:
        cpu: 2
        memory: '4Gi'
    containerPort: 8081
    pullPolicy: Always
    terminationGracePeriod: 120
    env:
      nexusDBName: nxrm3helmha
      nexusDBPort: 5432
      #\\nleakDetectionThreshold=20000
      install4jAddVmParams: '-Xms2g -Xmx2g -Dcom.redhat.fips=false -Dnexus.scripts.allowCreation=true -Dnexus.datastore.nexus.maximumPoolSize=10 -Dnexus.datastore.nexus.advanced=maxLifetime=30000'
      jdbcUrlParams: null # Must start with a '?' e.g. '?foo=bar&baz=foo'
      zeroDowntimeEnabled: true
  livenessProbe:
    failureThreshold: 6
    initialDelaySeconds: 120
    path: /
    periodSeconds: 60
  readinessProbe:
    failureThreshold: 6
    initialDelaySeconds: 120
    path: /
    periodSeconds: 60
  imagePullSecrets: {}

pvc:
  accessModes: ReadWriteMany
  storage: 10Gi
  volumeClaimTemplate:
    enabled: true
#  existingClaim: nexushelmha-nfs-pv-claim
# Check the value of `kubectl get sc`
storageClass:
  enabled: false
  name: nfs-client

service:  #Nexus Repo NodePort Service
  nexus:
    enabled: true
    # TODO: The name seems stopped working from 3.63?
    #name: 'nexus-service'
    type: NodePort
    protocol: TCP
    port: 8081
    targetPort: 8081

secret:
  secretProviderClass: 'secretProviderClass'
  provider: provider # e.g. aws, azure etc
  dbSecret:
    name: 'nxrm-db-secret'
    enabled: true
  db:
    user: 'nxrm3ha'
    userAlias: nxrm-db-user-alias
    password: 'nxrm3ha'
    passwordAlias: nxrm-db-password-alias
    host: '${DB_SERVER}'
    hostAlias: nxrm-db-host-alias
  nexusAdmin:
    name: 'nexusAdminPassword'
    alias: 'admin-nxrm-password-alias'
  nexusAdminSecret:
    enabled: true
    adminPassword: '${ADMIN_PWD}'
  license:
    name: nexus-repo-license.lic
    licenseSecret:
      enabled: true
      fileContentsBase64: '${LICENSE_B64}'
      mountPath: /var/nexus-repo-license

nexus:
  # This seems to work after 'uninstall' even "Ingress.networking.k8s.io "nxrm3ha-nxrm-ha-docker-5000" is invalid:..."
  docker:
    enabled: true
    type: NodePort
    protocol: TCP
    registries:
      - port: 5000
        targetPort: 4999
